{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('enron': conda)",
   "display_name": "Python 3.8.5 64-bit ('enron': conda)",
   "metadata": {
    "interpreter": {
     "hash": "28c752e26d1aa3388b746b19a93c9a1e7854e0c31672663278490571289c4318"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Projet Immothep\n",
    "\n",
    "Dans un premier temps nous avons décidé de nous concentrer seulement sur la ville de Bordeaux car c'est une grande ville française ne possèdant pas d'arrondissements.\n",
    "\n",
    "Du fait que cela soit simplement une ville et non pas la France entière nous pouvons observer plus simplement si la précision de notre algorithme est faussée par des données de notre jeu de données.\n",
    "\n",
    "Pour nettoyer notre jeu de données nous avons tout d'abord enlevé l'ensemble des colonnes non nécessaires à nos requêtes afin de ne conserver que les colonnes utiles (La nature de la mutation, la valeur fonciere, le code postal, le type de local, le nombre de pièces principales, la surface du terrain et la surface réelle bati)\n",
    "\n",
    "Concernant le type de local nous ne souhaitons estimer que les biens étant des appartements ou des maisons. Pour la nature de la mutation nous ne conservons que les biens ayant été mis en vente.\n",
    "\n",
    "Nous n'avons pas conservé les surfaces loi Carrez ainsi que les nombres de lots car cela faussait le jeu de données (Trop grande présence de lignes vides).\n",
    "\n",
    "En effet nous avons remarqué dans la documentation fournie par le site data.gouv.fr (D'où provient notre jeu de données) que la note descriptive de ce fichier nous indiquait que seuls les 5 premiers lots d'un lot plus important étaient restitués au sein du document.\n",
    "\n",
    "Le minimum légal en France pour la superficie d'un logement étant de 9m² nous avons donc décidé de supprimer de notre jeu de données tous les biens immobiliers ayant une surface réelle bati inférieure à 9m².\n",
    "\n",
    "Nous avons aussi enlevé de notre jeu de données les biens ayant 0 pièce principale car cela est illogique (Le minimum d'un bien étant une pièce principale)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPython extension reloading modules before user enters code.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from functions import download\n",
    "from functions import isolation_forest\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloader_object = download.downloader('./data')\n",
    "\n",
    "downloader_object.data_download('valeursfoncieres-2019.txt', 'https://www.data.gouv.fr/fr/datasets/r/3004168d-bec4-44d9-a781-ef16f41856a2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.read_csv('./data/RAW/valeursfoncieres-2019.txt', decimal = ',', sep = '|')\n",
    "df0.columns.tolist()\n",
    "df0 = df0.drop(columns = ['Code service CH',\n",
    " 'Reference document',\n",
    " '1 Articles CGI',\n",
    " '2 Articles CGI',\n",
    " '3 Articles CGI',\n",
    " '4 Articles CGI',\n",
    " '5 Articles CGI',\n",
    " 'No disposition',\n",
    " 'Date mutation',\n",
    " 'No voie',\n",
    " 'B/T/Q',\n",
    " 'Type de voie',\n",
    " 'Code voie',\n",
    " 'Voie',\n",
    " 'Code departement',\n",
    " 'Code commune',\n",
    " 'Prefixe de section',\n",
    " 'Section',\n",
    " 'No plan',\n",
    " 'No Volume',\n",
    " '1er lot',\n",
    " 'Surface Carrez du 1er lot',\n",
    " '2eme lot',\n",
    " 'Surface Carrez du 2eme lot',\n",
    " '3eme lot',\n",
    " 'Surface Carrez du 3eme lot',\n",
    " '4eme lot',\n",
    " 'Surface Carrez du 4eme lot',\n",
    " '5eme lot',\n",
    " 'Surface Carrez du 5eme lot',\n",
    " 'Nombre de lots',\n",
    " 'Code type local',\n",
    " 'Identifiant local',\n",
    " 'Nature culture',\n",
    " 'Nature culture speciale'])\n",
    "df = df0.copy() "
   ]
  },
  {
   "source": [
    "## Fonction preprocessing\n",
    "\n",
    "Cette fonction nous permet d'appliquer directement nos filtres à notre DataFrame."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.dropna(subset = ['Type local'])\n",
    "    df = df[df['Type local'].isin(['Maison','Appartement'])]\n",
    "    df = df[df['Commune'].isin(['BORDEAUX'])]\n",
    "    df = df[df['Nature mutation'].isin(['Vente'])]\n",
    "    df = df.dropna(subset = ['Code postal', 'Valeur fonciere'])\n",
    "    df = df[df[\"Surface reelle bati\"] > 8]\n",
    "    df = df[df[\"Nombre pieces principales\"] >= 1]\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocessing(df)\n",
    "df = df.drop(columns = ['Nature mutation', 'Commune', 'Type local'])\n",
    "df = df.to_csv('./data/CURATED/csv_clean_bordeaux.csv')"
   ]
  },
  {
   "source": [
    "## Utilisation de l'Isolation Forest\n",
    "\n",
    "Cet algorithme nous permet d'enlever toutes les valeurs aberrantes au sein de notre jeu de données. Elles sont notifiées en tant qu'anomalie."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isolation_forest.isolation_forest('./data/CURATED/csv_clean_bordeaux.csv', './data/CURATED/anomaly_csv_clean_bordeaux.csv')\n",
    "\n",
    "df_bordeaux = pd.read_csv('./data/CURATED/csv_clean_bordeaux.csv', sep = ',', usecols = ['Unnamed: 0', 'Valeur fonciere', 'Code postal', 'Surface reelle bati', 'Nombre pieces principales', 'Surface terrain'])\n",
    "df_bordeaux_anomaly = pd.read_csv('./data/CURATED/anomaly_csv_clean_bordeaux.csv', sep = ',')\n",
    "df_bordeaux_anomaly = df_bordeaux_anomaly.drop(columns = ['Unnamed: 0', 'Valeur fonciere', 'Code postal', 'Nombre pieces principales', 'Surface terrain', 'Surface reelle bati', 'scores'])\n",
    "df_bordeaux_anomaly = df_bordeaux_anomaly.rename(columns = {'Unnamed: 0.1':'Unnamed: 0'})\n",
    "df_bordeaux = df_bordeaux.merge(df_bordeaux_anomaly, how = 'left', on = 'Unnamed: 0')\n",
    "df_bordeaux = df_bordeaux.fillna(0)\n",
    "df_bordeaux = df_bordeaux[df_bordeaux[\"anomaly\"] == 0]\n",
    "df_bordeaux = df_bordeaux.drop(columns = ['Unnamed: 0', 'anomaly'])\n",
    "df_bordeaux['Valeur fonciere']  = df_bordeaux['Valeur fonciere'].astype(int)\n",
    "df_bordeaux['Code postal']  = df_bordeaux['Code postal'].astype(int)\n",
    "df_bordeaux['Surface reelle bati']  = df_bordeaux['Surface reelle bati'].astype(int)\n",
    "df_bordeaux['Nombre pieces principales']  = df_bordeaux['Nombre pieces principales'].astype(int)\n",
    "df_bordeaux['Surface terrain']  = df_bordeaux['Surface terrain'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bordeaux['Valeur fonciere'].plot(kind = 'box', title = 'Répartition des biens selon leur valeur foncière', figsize = (10, 8))\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Utilisation des quantiles\n",
    "\n",
    "Nous avons décidé d'enlever les 10% les plus hauts et les plus bas de notre colonne 'Valeur fonciere' afin d'enlever les dernières valeurs disparates de notre jeu de données."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bordeaux = df_bordeaux[df_bordeaux['Valeur fonciere'].between(df_bordeaux['Valeur fonciere'].quantile(.10), df_bordeaux['Valeur fonciere'].quantile(.90))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bordeaux['Valeur fonciere'].plot(kind = 'box', title = 'Répartition des biens selon leur valeur foncière', figsize = (10, 8))\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "Nous pouvons observer dans le boxplot ci-dessus que les valeurs éloignées ont été supprimé par l'utilisation des quantiles."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Calcul du Prix moyen au m² pour chaque code postal\n",
    "\n",
    "Nous pensons que le prix moyen au m² est plus intéressant pour évaluer la valeur d'un bien immobilier."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bordeaux = df_bordeaux.to_csv('./data/CURATED/csv_clean_bordeaux_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pd.read_csv('./data/CURATED/csv_clean_bordeaux_2.csv', usecols=['Valeur fonciere', 'Code postal', 'Surface reelle bati', 'Surface terrain'])\n",
    "df_clean['Prix moyen m²'] = df_clean['Valeur fonciere']/df_clean['Surface reelle bati']\n",
    "df_prix_moyen = df_clean.groupby('Code postal').mean('Prix moyen m²').drop(columns = ['Valeur fonciere', 'Surface reelle bati'])\n",
    "dict_prix_moyen = df_prix_moyen.to_dict()\n",
    "df_clean = df_clean.drop(columns = 'Prix moyen m²')\n",
    "df_prix_moyen = df_prix_moyen.drop(columns = 'Surface terrain')\n",
    "df_clean = df_clean.merge(df_prix_moyen, how = 'left', on = 'Code postal')\n",
    "df_clean = df_clean.drop(columns = 'Code postal')\n",
    "df_clean = df_clean.to_csv('./data/CURATED/csv_clean_bordeaux_final.csv')"
   ]
  },
  {
   "source": [
    "## Utilisation de Random Forest Regressor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv(\"./data/CURATED/csv_clean_bordeaux_final.csv\", usecols = ['Valeur fonciere', 'Surface reelle bati', 'Surface terrain', 'Prix moyen m²'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrice_correlation = features.corr().round(1)\n",
    "sns.heatmap(data = matrice_correlation, annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels are the values we want to predict\n",
    "labels = np.array(features['Valeur fonciere'])\n",
    "# Remove the labels from the features\n",
    "features = features.drop('Valeur fonciere', axis = 1)\n",
    "# Saving feature names for later use\n",
    "feature_list = list(features.columns) \n",
    "# Convert to numpy array\n",
    "features = np.array(features)\n",
    "# Split the data into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 50, random_state = 1)\n",
    "# Train the model on training data\n",
    "rf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(test_features)\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - test_labels)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'euros.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / test_labels)\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}